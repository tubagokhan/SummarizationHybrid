{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPJHI4xCvSi2EdwFj6xM9ZJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tubagokhan/SummarizationHybrid/blob/main/GraphandCluster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install torch\n",
        "!pip install scikit-learn\n",
        "!pip install matplotlib\n",
        "!pip install sentence_transformers\n",
        "!pip install py-rouge==1.1"
      ],
      "metadata": {
        "id": "ThO6v86y2WAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from nltk import sent_tokenize,word_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import re\n",
        "\n",
        "from datasets import load_dataset\n",
        "from scipy.spatial import distance\n",
        "\n",
        "import math\n",
        "from math import*\n",
        "\n",
        "import rouge\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "import time\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()"
      ],
      "metadata": {
        "id": "CZfgFyRF3kb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing method\n",
        "def preprocess_corpus(text):\n",
        "    # Remove special characters and extra whitespaces\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s.]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "EETObHuc9vy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLUSTER"
      ],
      "metadata": {
        "id": "9jdUGsPfHSlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function calculates the optimal number of clusters using the elbow method. The function plots the elbow curve, which shows the inertia values for different cluster numbers. The user can visually inspect the plot to determine the elbow point, indicating the optimal number of clusters.\n",
        "def find_optimum_clusters(data, max_clusters):\n",
        "    inertias = []\n",
        "    for k in range(1, max_clusters + 1):\n",
        "        kmeans = KMeans(n_clusters=k, random_state=0).fit(data)\n",
        "        inertias.append(kmeans.inertia_)\n",
        "\n",
        "    # Plotting the elbow curve\n",
        "    #plt.plot(range(1, max_clusters + 1), inertias)\n",
        "    #plt.xlabel(\"Number of Clusters\")\n",
        "    #plt.ylabel(\"Inertia\")\n",
        "    #plt.title(\"Elbow Curve\")\n",
        "    #plt.show()\n",
        "\n",
        "    # Calculate the optimal number of clusters using the elbow method\n",
        "    diff = np.diff(inertias)\n",
        "    acceleration = np.diff(diff)\n",
        "    opt_cluster_num = acceleration.argmin() + 2  # Adding 2 to get the index of the minimum acceleration\n",
        "    return opt_cluster_num"
      ],
      "metadata": {
        "id": "R5n2jcYW7Z_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHQjZaUt2Lax"
      },
      "outputs": [],
      "source": [
        "def createSummaryUsingKMeans(corpus, modelName):\n",
        "    sentences = sent_tokenize(corpus)\n",
        "    model = SentenceTransformer(modelName)\n",
        "    sentence_embeddings = model.encode(sentences)\n",
        "\n",
        "    optimum_clusters = find_optimum_clusters(sentence_embeddings, int(len(sentences) / 3))\n",
        "    print(\"Optimum cluster number:\", optimum_clusters)\n",
        "\n",
        "    # Perform kmean clustering\n",
        "    kmeans = KMeans(n_clusters=optimum_clusters, random_state=0, n_init='auto').fit(sentence_embeddings)\n",
        "\n",
        "    chosen_sentence_indexes=[]\n",
        "    for cluster_id in range(optimum_clusters):\n",
        "        cluster_indices = np.where(kmeans.labels_ == cluster_id)[0]\n",
        "        sorted_cluster_indices = sorted(cluster_indices, key=lambda x: sentences[x])\n",
        "        chosen_sentence_index = sorted_cluster_indices[0]  # Select the first sentence from the sorted indices\n",
        "        chosen_sentence_indexes.append(chosen_sentence_index)\n",
        "\n",
        "    sorted_indexes=sorted(chosen_sentence_indexes)\n",
        "\n",
        "    chosen_sentences = []\n",
        "    for chosen_sentence_index in sorted_indexes:\n",
        "        chosen_sentences.append(sentences[chosen_sentence_index])\n",
        "\n",
        "    summary = \" \".join(chosen_sentences)\n",
        "\n",
        "    return summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRAPH NODE WEIGHTS"
      ],
      "metadata": {
        "id": "e7kCMPHiHVm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def allCorpusSentenceRanking(tokenizedCorpus,corpus):\n",
        "    sentenceRankList=[]\n",
        "    for i in range(len(tokenizedCorpus)):\n",
        "        value=sentenceRanking(tokenizedCorpus[i],i,corpus)\n",
        "        value=round(value,5)\n",
        "        sentenceRankList.append(value)\n",
        "    return sentenceRankList\n",
        "\n",
        "def sentenceRanking(sentence,location,corpus):\n",
        "    value=0\n",
        "    value=sentencePosition(sentence,location,corpus)\n",
        "    value=value+sentenceLength(sentence,corpus)\n",
        "    value=value+properNoun(sentence,corpus)\n",
        "    value=value+numericalToken(sentence,corpus)\n",
        "    return value\n",
        "\n",
        "def textWordCount(Text):\n",
        "    number_of_words = word_tokenize(Text)\n",
        "    count=(len(number_of_words))\n",
        "    return count\n",
        "\n",
        "def textSentenceCount(Text):\n",
        "    number_of_sentences = sent_tokenize(Text)\n",
        "    count=(len(number_of_sentences))\n",
        "    return count\n",
        "\n",
        "def longestSentenceLenght(Text):\n",
        "    text=sent_tokenize(Text)\n",
        "    temp=0\n",
        "    for i in range(len(text)):\n",
        "        if temp<textWordCount(text[i]):\n",
        "            temp=textWordCount(text[i])\n",
        "    return temp\n",
        "\n",
        "def sentencePosition(sentence,location,corpus):\n",
        "    N=textSentenceCount(corpus)\n",
        "    if location+1 == N:\n",
        "        return 1.0\n",
        "    elif location==0:\n",
        "        return 1.0\n",
        "    else:\n",
        "        value=(N-location)/N\n",
        "        return value\n",
        "\n",
        "def sentenceLength(sentence,corpus):\n",
        "    return textWordCount(sentence)/longestSentenceLenght(corpus)\n",
        "\n",
        "def properNoun(sentence,corpus):\n",
        "    text = nltk.word_tokenize(sentence)\n",
        "    tagged=nltk.pos_tag(text)\n",
        "    noProperNoun=0\n",
        "    #print(tagged)\n",
        "    for word in tagged:\n",
        "        if word[1]=='NNP':\n",
        "            noProperNoun=noProperNoun+1\n",
        "    #print(noProperNoun)\n",
        "    return noProperNoun/len(text)\n",
        "\n",
        "def numericalToken(sentence,corpus):\n",
        "    text = nltk.word_tokenize(sentence)\n",
        "    tagged=nltk.pos_tag(text)\n",
        "    noNumericalToken=0\n",
        "    #print(tagged)\n",
        "    for word in tagged:\n",
        "        if word[1]=='CD':\n",
        "            noNumericalToken=noNumericalToken+1\n",
        "  #print(noProperNoun)\n",
        "    return 1-(noNumericalToken/len(text))"
      ],
      "metadata": {
        "id": "bU3377uoG07l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRAPH"
      ],
      "metadata": {
        "id": "ZEayEw4EHd5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
        "\n",
        "def createGraph(sentences, modelName):\n",
        "    model = SentenceTransformer(modelName)\n",
        "    sentence_embeddings = model.encode(sentences)\n",
        "    sentenceGraph =np.zeros((len(sentences), len(sentences)))\n",
        "    temp = np.arange(len(sentences))\n",
        "    for x in range(len(sentences)):\n",
        "        newTemp= np.delete(temp, x)\n",
        "        for y in newTemp:\n",
        "            similarity= cosine(sentence_embeddings[x],sentence_embeddings[y]) # You can change the vector similarity measurement method used when creating graphs. Cosine, euclidean, manhattan and minkowski methods are defined.\n",
        "            sentenceGraph[x][y]=abs(similarity)\n",
        "    return sentenceGraph\n",
        "\n",
        "def findHighestSimilarityRank(similarityMatrix, initialRank):\n",
        "    newRank=[0] * len(similarityMatrix)\n",
        "    temp=0\n",
        "    for i in range(len(similarityMatrix)):\n",
        "        for j in range(len(similarityMatrix)):\n",
        "            temp=temp+similarityMatrix[i][j] # sum of total similarity of sentences\n",
        "        newRank[i]=temp*initialRank[i]\n",
        "        temp=0\n",
        "\n",
        "    return newRank\n",
        "\n",
        "def findHighestSimilarityRankNoWRANK(similarityMatrix, initialRank):\n",
        "\n",
        "    nodeWeightLSARanks = initialRank\n",
        "\n",
        "    for x in range(len(initialRank)):\n",
        "        for y in range(len(initialRank)):\n",
        "            temp=initialRank[x]+initialRank[y]\n",
        "            similarityMatrix[x][y]=similarityMatrix[x][y]+temp*0.01\n",
        "    G = nx.Graph()\n",
        "    for x in range(len(initialRank)):\n",
        "        for y in range(len(initialRank)):\n",
        "            G.add_node(x)\n",
        "            G.add_edge(x, y, weight=similarityMatrix[x,y] )\n",
        "    eigenVectorCentrality = nx.eigenvector_centrality(G, max_iter=100, tol=1.0e-6, nstart=None, weight='weight')\n",
        "\n",
        "    edgeWeightEigenVectorRanks=[0]*len(eigenVectorCentrality)\n",
        "    for i in range (len(eigenVectorCentrality)):\n",
        "        edgeWeightEigenVectorRanks[i]=eigenVectorCentrality[i]\n",
        "\n",
        "    return edgeWeightEigenVectorRanks\n",
        "\n",
        "def createSummaryUsingGraph(corpus,modelName):\n",
        "  sentences = sent_tokenize(corpus)\n",
        "  summaryAmmount= int(len(sentences)*0.2)\n",
        "  initialRank=allCorpusSentenceRanking(sentences,corpus)\n",
        "  similarityMatrix=createGraph(sentences,modelName)\n",
        "  sentencesRank=findHighestSimilarityRankNoWRANK(similarityMatrix, initialRank)\n",
        "\n",
        "  temp=sorted(sentencesRank)\n",
        "  threshold=temp[-summaryAmmount]\n",
        "  summarySentencesIndexes=[]\n",
        "\n",
        "  for i in range(len(sentencesRank)):\n",
        "      if sentencesRank[i]>=threshold:\n",
        "        summarySentencesIndexes.append(i)\n",
        "\n",
        "  #print(summarySentencesIndexes)\n",
        "  summary=\"\"\n",
        "  for i in range(len(summarySentencesIndexes)):\n",
        "    summary=summary + ' ' +sentences[summarySentencesIndexes[i]]\n",
        "  return summary\n"
      ],
      "metadata": {
        "id": "EL-P1bqbHHGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MAIN\n",
        "modelName='all-mpnet-base-v2'\n",
        "\n",
        "#dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
        "#corpus = dataset['train']['article'][50]\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"ccdv/govreport-summarization\")\n",
        "\n",
        "\n",
        "startTimeforOverall = time.time()\n",
        "all_intermediate_summary=[]\n",
        "all_summary=[]\n",
        "all_goldstandart=[]\n",
        "\n",
        "N=100\n",
        "startN=0\n",
        "\n",
        "for d in range(N):\n",
        "    startTimeforDocument = time.time()\n",
        "    print(\"Document:\",startN+d+1)\n",
        "    corpus=dataset['train']['report'][startN+d]\n",
        "    corpus = preprocess_corpus(corpus)\n",
        "    #print(corpus)\n",
        "    print(\"Document sentence number:\",textSentenceCount(corpus))\n",
        "    all_goldstandart.append(dataset['train']['summary'][startN+d])\n",
        "\n",
        "    intermadiatesummary=createSummaryUsingGraph(corpus, modelName)\n",
        "    print(\"Intermediate summary sentence number:\",textSentenceCount(intermadiatesummary))\n",
        "    all_intermediate_summary.append(intermadiatesummary)\n",
        "\n",
        "\n",
        "    summary=createSummaryUsingKMeans(intermadiatesummary, modelName)\n",
        "    print(\"Final summary sentence number:\",textSentenceCount(summary))\n",
        "    all_summary.append(summary)\n",
        "\n",
        "    elapsedTimeforDocument = time.time() - startTimeforDocument\n",
        "    elapsedTimeforAll = time.time() - startTimeforOverall\n",
        "    print('Document processing time: '+time.strftime(\"%M:%S\", time.gmtime(elapsedTimeforDocument)))\n",
        "    print('Total processing time: '+time.strftime(\"%d:%H:%M:%S\", time.gmtime(elapsedTimeforAll)))\n",
        "\n",
        "    print(\"----------------------------------\")\n"
      ],
      "metadata": {
        "id": "BrKrlYxN3auN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_results(m, p, r, f):\n",
        "    return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(m, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)\n",
        "\n",
        "def rougeEvaluation(all_hypothesis, all_references):\n",
        "\n",
        "    for aggregator in ['Avg']:\n",
        "        print('Evaluation with {}'.format(aggregator))\n",
        "        apply_avg = aggregator == 'Avg'\n",
        "        apply_best = aggregator == 'Best'\n",
        "\n",
        "        evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
        "                               max_n=4,\n",
        "                               limit_length=False,\n",
        "                               length_limit=1000,\n",
        "                               length_limit_type='words',\n",
        "                               apply_avg=apply_avg,\n",
        "                               apply_best=apply_best,\n",
        "                               alpha=0.2, # Default F1_score\n",
        "                               weight_factor=1.2,\n",
        "                               stemming=True)\n",
        "\n",
        "        scores = evaluator.get_scores(all_hypothesis, all_references)\n",
        "\n",
        "        for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
        "            if not apply_avg and not apply_best: # value is a type of list as we evaluate each summary vs each reference\n",
        "                for hypothesis_id, results_per_ref in enumerate(results):\n",
        "                    nb_references = len(results_per_ref['p'])\n",
        "                    for reference_id in range(nb_references):\n",
        "                        print('\\tHypothesis #{} & Reference #{}: '.format(hypothesis_id, reference_id))\n",
        "                        print('\\t' + prepare_results(metric,results_per_ref['p'][reference_id], results_per_ref['r'][reference_id], results_per_ref['f'][reference_id]))\n",
        "                print()\n",
        "            else:\n",
        "                print(prepare_results(metric, results['p'], results['r'], results['f']))\n",
        "        print()"
      ],
      "metadata": {
        "id": "po9dIR53OKQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rougeEvaluation(all_intermediate_summary, all_goldstandart)"
      ],
      "metadata": {
        "id": "YdrNFPg4OnPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rougeEvaluation(all_summary, all_goldstandart)"
      ],
      "metadata": {
        "id": "EBXQ0G2-PnlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Play an audio beep. Any audio URL will do.\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "metadata": {
        "id": "q7CA0TIIXmxP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}